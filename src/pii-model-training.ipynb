{"cells":[{"cell_type":"markdown","metadata":{},"source":["# PII Model Training Notebook"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:48:08.365802Z","iopub.status.busy":"2024-02-26T13:48:08.365424Z","iopub.status.idle":"2024-02-26T13:48:08.371293Z","shell.execute_reply":"2024-02-26T13:48:08.370349Z","shell.execute_reply.started":"2024-02-26T13:48:08.365774Z"},"trusted":true},"outputs":[],"source":["import argparse\n","from itertools import chain\n","from functools import partial\n","\n","import torch\n","from transformers import AutoTokenizer, Trainer, TrainingArguments\n","from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n","import evaluate\n","from datasets import Dataset, features\n","import numpy as np\n","import pandas as pd\n","import json"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:44:07.873466Z","iopub.status.busy":"2024-02-26T13:44:07.873138Z","iopub.status.idle":"2024-02-26T13:44:07.891071Z","shell.execute_reply":"2024-02-26T13:44:07.890120Z","shell.execute_reply.started":"2024-02-26T13:44:07.873437Z"},"trusted":true},"outputs":[],"source":["TRAINING_MODEL_PATH = \"allenai/longformer-base-4096\"\n","TRAINING_MAX_LENGTH = 1024\n","STRIDE = 384\n","OUTPUT_DIR = \"output-longformer\"\n","RANDOM_STATE = 29"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:44:07.894291Z","iopub.status.busy":"2024-02-26T13:44:07.893976Z","iopub.status.idle":"2024-02-26T13:44:15.429846Z","shell.execute_reply":"2024-02-26T13:44:15.428849Z","shell.execute_reply.started":"2024-02-26T13:44:07.894262Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of samples in data: 21672\n"]}],"source":["data = json.load(open(\"../Dataset/data.json\"))\n","print(\"Number of samples in data:\", len(data))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:44:15.431480Z","iopub.status.busy":"2024-02-26T13:44:15.431114Z","iopub.status.idle":"2024-02-26T13:44:15.449339Z","shell.execute_reply":"2024-02-26T13:44:15.448321Z","shell.execute_reply.started":"2024-02-26T13:44:15.431444Z"},"trusted":true},"outputs":[],"source":["train, valid = [], []\n","\n","for row in data:\n","    if row[\"valid\"]: valid.append(row)\n","    else: train.append(row)\n","        \n","print(\"Samples in training data:\", len(train))\n","print(\"Samples in validation data:\", len(valid))"]},{"cell_type":"markdown","metadata":{},"source":["### Data Selection and Mapping\n","\n","Negative sampling training data to get better results. (As suggested by Valentin)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:44:15.451114Z","iopub.status.busy":"2024-02-26T13:44:15.450730Z","iopub.status.idle":"2024-02-26T13:44:20.197121Z","shell.execute_reply":"2024-02-26T13:44:20.196102Z","shell.execute_reply.started":"2024-02-26T13:44:15.451077Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive samples: 13975\n","Negative samples: 6347\n"]}],"source":["p=[] # positive samples (contain relevant labels)\n","n=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\n","\n","for d in train:\n","    if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n","    else: n.append(d)\n","        \n","        \n","print(f\"Positive samples: {len(p)}\")\n","print(f\"Negative samples: {len(n)}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:44:20.198629Z","iopub.status.busy":"2024-02-26T13:44:20.198258Z","iopub.status.idle":"2024-02-26T13:44:20.205384Z","shell.execute_reply":"2024-02-26T13:44:20.204422Z","shell.execute_reply.started":"2024-02-26T13:44:20.198603Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Down sampled training: 16090\n"]}],"source":["train = p + n[:len(n) // 3]\n","print(f\"Down sampled training: {len(train)}\")"]},{"cell_type":"markdown","metadata":{},"source":["### ðŸ“œ Data Tokenization"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:44:20.206963Z","iopub.status.busy":"2024-02-26T13:44:20.206640Z","iopub.status.idle":"2024-02-26T13:44:20.406953Z","shell.execute_reply":"2024-02-26T13:44:20.405967Z","shell.execute_reply.started":"2024-02-26T13:44:20.206931Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n"]}],"source":["all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}\n","\n","target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]\n","\n","print(id2label)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:44:20.408781Z","iopub.status.busy":"2024-02-26T13:44:20.408385Z","iopub.status.idle":"2024-02-26T13:44:20.420752Z","shell.execute_reply":"2024-02-26T13:44:20.419848Z","shell.execute_reply.started":"2024-02-26T13:44:20.408747Z"},"trusted":true},"outputs":[],"source":["def rebuild_from_example(example):\n","    text, labels, token_map = [], [], []\n","\n","    for idx, (t, l, ws) in enumerate(zip(\n","        example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]\n","    )):\n","        text.append(t)\n","        token_map.extend([idx] * len(t))\n","        labels.extend([l] * len(t))\n","\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","            token_map.append(-1)\n","            \n","    labels = np.array(labels)\n","    text = \"\".join(text)\n","    \n","    return text, labels, token_map"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:44:20.430897Z","iopub.status.busy":"2024-02-26T13:44:20.430628Z","iopub.status.idle":"2024-02-26T13:44:20.441451Z","shell.execute_reply":"2024-02-26T13:44:20.440531Z","shell.execute_reply.started":"2024-02-26T13:44:20.430874Z"},"trusted":true},"outputs":[],"source":["def tokenize(example, tokenizer, label2id, max_length):\n","    text, labels, token_map = rebuild_from_example(example)\n","\n","    # actual tokenization\n","    tokenized = tokenizer(text, \n","                          return_offsets_mapping=True, \n","                          max_length=max_length, \n","                          truncation=True)\n","    \n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0:\n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        token_labels.append(label2id[labels[start_idx]])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {\n","        **tokenized, \n","        \"labels\": token_labels, \n","        \"length\": length,\n","        \"token_map\": token_map\n","    }"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:44:20.442895Z","iopub.status.busy":"2024-02-26T13:44:20.442589Z","iopub.status.idle":"2024-02-26T13:44:20.453769Z","shell.execute_reply":"2024-02-26T13:44:20.452843Z","shell.execute_reply.started":"2024-02-26T13:44:20.442871Z"},"trusted":true},"outputs":[],"source":["def create_dict(data):\n","    keys = [\"full_text\", \"document\", \"tokens\", \"trailing_whitespace\", \"labels\", \"token_indices\"]\n","    \n","    # Initialize each key to have the same number of elements\n","    # as the number of rows in `data`\n","    output = {key: [None] * len(data) for key in keys}\n","    \n","    # Assign values to the dictionary\n","    for idx, row in enumerate(data):\n","        for key in keys:\n","            output[key][idx] = row[key]\n","    \n","    return output"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:47:44.092073Z","iopub.status.busy":"2024-02-26T13:47:44.091706Z","iopub.status.idle":"2024-02-26T13:47:50.877091Z","shell.execute_reply":"2024-02-26T13:47:50.875948Z","shell.execute_reply.started":"2024-02-26T13:47:44.092045Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/media/ishfar/New Volume/Studies/Projects/Kaggle/PII_Detection/venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"681010c2c26942e2a28f3ba6a3eab8ac","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=6):   0%|          | 0/16090 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ebdf27b5c4c40bdb452eb0b2e4beb53","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=6):   0%|          | 0/1350 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n","\n","train_ds = Dataset.from_dict(create_dict(train))\n","train_ds = train_ds.map(tokenize, \n","                        fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAINING_MAX_LENGTH}, \n","                        num_proc=6)\n","\n","valid_ds = Dataset.from_dict(create_dict(valid))\n","valid_ds = valid_ds.map(tokenize, \n","                        fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAINING_MAX_LENGTH}, \n","                        num_proc=6)"]},{"cell_type":"markdown","metadata":{},"source":["## ðŸ“Š Metrics \n","\n","https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/470024"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:47:53.076688Z","iopub.status.busy":"2024-02-26T13:47:53.075971Z","iopub.status.idle":"2024-02-26T13:47:53.089624Z","shell.execute_reply":"2024-02-26T13:47:53.088506Z","shell.execute_reply.started":"2024-02-26T13:47:53.076654Z"},"trusted":true},"outputs":[],"source":["from typing import Dict\n","\n","class PRFScore:\n","    \"\"\"A precision / recall / F score.\"\"\"\n","\n","    def __init__(self, *, tp: int = 0, fp: int = 0, fn: int = 0) -> None:\n","        self.tp, self.fp, self.fn = tp, fp, fn\n","\n","    def __len__(self) -> int:\n","        return self.tp + self.fp + self.fn\n","\n","    def __iadd__(self, other):  # in-place add\n","        self.tp += other.tp\n","        self.fp += other.fp\n","        self.fn += other.fn\n","        return self\n","\n","    def __add__(self, other):\n","        return PRFScore(\n","            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n","        )\n","\n","    def score_set(self, cand: set, gold: set) -> None:\n","        self.tp += len(cand.intersection(gold))\n","        self.fp += len(cand - gold)\n","        self.fn += len(gold - cand)\n","\n","    @property\n","    def precision(self) -> float:\n","        return self.tp / (self.tp + self.fp + 1e-100)\n","\n","    @property\n","    def recall(self) -> float:\n","        return self.tp / (self.tp + self.fn + 1e-100)\n","\n","    @property\n","    def f1(self) -> float:\n","        p, r = self.precision, self.recall\n","        return 2 * ((p * r) / (p + r + 1e-100))\n","\n","    @property\n","    def f5(self) -> float:\n","        beta, p, r = 5, self.precision, self.recall\n","        fbeta = (1 + (beta**2)) * p * r / ((beta**2) * p + r + 1e-100)\n","        return fbeta\n","\n","    def to_dict(self) -> Dict[str, float]:\n","        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:47:53.276104Z","iopub.status.busy":"2024-02-26T13:47:53.275757Z","iopub.status.idle":"2024-02-26T13:47:53.289552Z","shell.execute_reply":"2024-02-26T13:47:53.288549Z","shell.execute_reply.started":"2024-02-26T13:47:53.276078Z"},"trusted":true},"outputs":[],"source":["def parse_predictions(predictions, id2label, ds, threshold=0.9):\n","    pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis=2).reshape(\n","        predictions.shape[0], predictions.shape[1], 1\n","    )\n","    preds = predictions.argmax(-1)\n","    preds_without_O = pred_softmax[:, :, :12].argmax(-1)\n","    O_preds = pred_softmax[:, :, 12]\n","    preds_final = np.where(O_preds < threshold, preds_without_O, preds)\n","    \n","    pairs = []\n","    row, document, token, label, token_str = [], [], [], [], []\n","    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(\n","        zip(\n","            preds_final,\n","            ds[\"token_map\"],\n","            ds[\"offset_mapping\"],\n","            ds[\"tokens\"],\n","            ds[\"document\"],\n","            ds[\"token_indices\"],\n","        )\n","    ):\n","\n","        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n","            label_pred = id2label[token_pred]\n","\n","            if start_idx + end_idx == 0:\n","                continue\n","\n","            if token_map[start_idx] == -1:\n","                start_idx += 1\n","\n","            # ignore \"\\n\\n\"\n","            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n","                start_idx += 1\n","\n","            if start_idx >= len(token_map):\n","                break\n","\n","            original_token_id = token_map[start_idx]\n","            token_id = indices[original_token_id]\n","            \n","            # ignore \"O\" predictions and whitespace preds\n","            if label_pred != \"O\" and token_id != -1:\n","                pair=(doc, token_id)\n","\n","                if pair not in pairs:\n","                    row.append(i)\n","                    document.append(doc)\n","                    token.append(token_id)\n","                    label.append(label_pred)\n","                    token_str.append(tokens[original_token_id])\n","                    pairs.append(pair)\n","                    \n","    df = pd.DataFrame(\n","        {\n","            \"eval_row\": row,\n","            \"document\": document,\n","            \"token\": token,\n","            \"label\": label,\n","            \"token_str\": token_str,\n","        }\n","    )\n","\n","    df = df.drop_duplicates().reset_index(drop=True)\n","\n","    df[\"row_id\"] = list(range(len(df)))\n","    return df"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:47:53.499863Z","iopub.status.busy":"2024-02-26T13:47:53.499465Z","iopub.status.idle":"2024-02-26T13:47:53.513094Z","shell.execute_reply":"2024-02-26T13:47:53.512194Z","shell.execute_reply.started":"2024-02-26T13:47:53.499836Z"},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","\n","def compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n","    predictions, labels = p\n","    \n","    pred_df = parse_predictions(predictions, id2label, valid_ds, threshold=threshold)\n","    \n","    references = {\n","        (row.document, row.token, row.label) # TODO: Change to pair\n","        for row in valid_df.itertuples()\n","    }\n","    predictions = {\n","        (row.document, row.token, row.label) # TODO: Change to pair\n","        for row in pred_df.itertuples()\n","    }\n","    \n","    score_per_type = defaultdict(PRFScore)\n","    references = set(references)\n","    \n","    for ex in predictions:\n","        pred_type = ex[-1] # (Document, token, label)\n","        \n","        if pred_type != \"O\":\n","            pred_type = pred_type[2:] # Discard B- and I- prefix\n","            \n","        if pred_type not in score_per_type:\n","            score_per_type[pred_type] = PRFScore()\n","            \n","        if ex in references:\n","            score_per_type[pred_type].tp += 1\n","            references.remove(ex)\n","        else:\n","            score_per_type[pred_type].fp += 1\n","            \n","    for _, _, ref_type in references: # Remaining labels not predicted\n","        if pred_type != \"O\":\n","            pred_type = pred_type[2:] # Discard B- and I- prefix\n","        \n","        if pred_type not in score_per_type:\n","            score_per_type[pred_type] = PRFScore()\n","            \n","        score_per_type[pred_type].fn += 1\n","        \n","    totals = PRFScore()\n","    \n","    for prf in score_per_type.values():\n","        totals += prf\n","        \n","    results = {\n","        \"ents_p\": totals.precision,\n","        \"ents_r\": totals.recall,\n","        \"ents_f5\": totals.f5,\n","        \"ents_per_type\": {\n","            k: v.to_dict() for k, v in score_per_type.items() if k != \"O\"\n","        },\n","    }\n","    \n","    final_results = {}\n","    for key, value in results.items():\n","        if isinstance(value, dict):\n","            for n, v in value.items():\n","                if isinstance(v, dict):\n","                    for n2, v2 in v.items():\n","                        final_results[f\"{key}_{n}_{n2}\"] = v2\n","                else:\n","                    final_results[f\"{key}_{n}\"] = v\n","        else:\n","            final_results[key] = value\n","\n","    return final_results"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:48:12.538427Z","iopub.status.busy":"2024-02-26T13:48:12.538070Z","iopub.status.idle":"2024-02-26T13:48:13.028430Z","shell.execute_reply":"2024-02-26T13:48:13.027633Z","shell.execute_reply.started":"2024-02-26T13:48:12.538399Z"},"trusted":true},"outputs":[],"source":["def get_reference_df(valid):\n","    raw_df = pd.DataFrame(valid)\n","    ref_df = raw_df[[\"document\", \"tokens\", \"labels\"]].copy()\n","    ref_df = (\n","        ref_df.explode([\"tokens\", \"labels\"])\n","        .reset_index(drop=True)\n","        .rename(columns={\"tokens\": \"token\", \"labels\": \"label\"})\n","    )\n","    ref_df[\"token\"] = ref_df.groupby(\"document\").cumcount()\n","\n","    reference_df = ref_df[ref_df[\"label\"] != \"O\"].copy()\n","    reference_df = reference_df.reset_index().rename(columns={\"index\": \"row_id\"})\n","    reference_df = reference_df[[\"row_id\", \"document\", \"token\", \"label\"]].copy()\n","\n","    return reference_df\n","\n","\n","reference_df = get_reference_df(valid)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:48:15.138244Z","iopub.status.busy":"2024-02-26T13:48:15.137547Z","iopub.status.idle":"2024-02-26T13:48:18.983390Z","shell.execute_reply":"2024-02-26T13:48:18.982630Z","shell.execute_reply.started":"2024-02-26T13:48:15.138213Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForTokenClassification.from_pretrained(\n","    TRAINING_MODEL_PATH,\n","    num_labels=len(all_labels),\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True\n",")\n","collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:48:23.910080Z","iopub.status.busy":"2024-02-26T13:48:23.909716Z","iopub.status.idle":"2024-02-26T13:48:23.943348Z","shell.execute_reply":"2024-02-26T13:48:23.942295Z","shell.execute_reply.started":"2024-02-26T13:48:23.910052Z"},"trusted":true},"outputs":[],"source":["args = TrainingArguments(\n","    output_dir=OUTPUT_DIR, \n","    fp16=True,\n","    learning_rate=1e-5,\n","    num_train_epochs=0.001,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=2,\n","    report_to=\"none\",\n","    evaluation_strategy=\"epoch\",\n","    do_eval=True,\n","    save_total_limit=1,\n","    logging_steps=10,\n","    lr_scheduler_type='cosine',\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    warmup_ratio=0.1,\n","    weight_decay=0.01\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Set \"O\" tokens to have a very small weight"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:48:25.441724Z","iopub.status.busy":"2024-02-26T13:48:25.441353Z","iopub.status.idle":"2024-02-26T13:48:25.580516Z","shell.execute_reply":"2024-02-26T13:48:25.579491Z","shell.execute_reply.started":"2024-02-26T13:48:25.441696Z"},"trusted":true},"outputs":[],"source":["O_WEIGHTS = 0.01\n","class_weights = torch.tensor([1.0] * 12 + [O_WEIGHTS]).to(\"cuda\")"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Trainer"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:48:27.577728Z","iopub.status.busy":"2024-02-26T13:48:27.577349Z","iopub.status.idle":"2024-02-26T13:48:27.582409Z","shell.execute_reply":"2024-02-26T13:48:27.581203Z","shell.execute_reply.started":"2024-02-26T13:48:27.577698Z"},"trusted":true},"outputs":[],"source":["from torch.nn import CrossEntropyLoss"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:48:28.345691Z","iopub.status.busy":"2024-02-26T13:48:28.345349Z","iopub.status.idle":"2024-02-26T13:48:28.955106Z","shell.execute_reply":"2024-02-26T13:48:28.954075Z","shell.execute_reply.started":"2024-02-26T13:48:28.345665Z"},"trusted":true},"outputs":[],"source":["class CustomTrainer(Trainer):\n","    def __init__(self, *args, class_weights=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        # Assuming class_weights is a Tensor of weights for each class\n","        self.class_weights = class_weights\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # Extract labels\n","        labels = inputs.pop(\"labels\")\n","        \n","        # Forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        \n","        # Reshape for loss calculation\n","        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n","        \n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            loss = self.label_smoother(outputs, inputs)\n","        else:\n","            loss = loss_fct(\n","                logits.view(-1, self.model.config.num_labels), labels.view(-1)\n","            )\n","\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:48:30.891495Z","iopub.status.busy":"2024-02-26T13:48:30.891126Z","iopub.status.idle":"2024-02-26T13:48:31.102103Z","shell.execute_reply":"2024-02-26T13:48:31.101253Z","shell.execute_reply.started":"2024-02-26T13:48:30.891452Z"},"trusted":true},"outputs":[],"source":["trainer = CustomTrainer(\n","    model=model, \n","    args=args, \n","    train_dataset=train_ds,\n","    eval_dataset=valid_ds,\n","    data_collator=collator, \n","    tokenizer=tokenizer,\n","    compute_metrics=partial(\n","        compute_metrics,\n","        id2label=id2label,\n","        valid_ds=valid_ds,\n","        valid_df=reference_df,\n","        threshold=0.9,\n","    ),\n","    class_weights=class_weights\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T13:48:52.115528Z","iopub.status.busy":"2024-02-26T13:48:52.114702Z"},"trusted":true},"outputs":[],"source":["train_result = trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["### Saving Model and Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-26T13:47:36.795833Z","iopub.status.idle":"2024-02-26T13:47:36.796184Z","shell.execute_reply":"2024-02-26T13:47:36.796032Z","shell.execute_reply.started":"2024-02-26T13:47:36.796013Z"},"trusted":true},"outputs":[],"source":["trainer.save_model(\"longformer_base_4096\")\n","tokenizer.save_pretrained(\"longformer_base_4096\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-26T13:47:36.797446Z","iopub.status.idle":"2024-02-26T13:47:36.797784Z","shell.execute_reply":"2024-02-26T13:47:36.797642Z","shell.execute_reply.started":"2024-02-26T13:47:36.797628Z"},"trusted":true},"outputs":[],"source":["metrics = train_result.metrics\n","max_train_samples = len(train_ds)\n","metrics[\"train_samples\"] = min(max_train_samples, len(train_ds))\n","\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-26T13:47:36.799306Z","iopub.status.idle":"2024-02-26T13:47:36.799757Z","shell.execute_reply":"2024-02-26T13:47:36.799552Z","shell.execute_reply.started":"2024-02-26T13:47:36.799524Z"},"trusted":true},"outputs":[],"source":["metrics = trainer.evaluate()\n","max_val_samples = len(valid_ds)\n","metrics[\"eval_samples\"] = min(max_val_samples, len(valid_ds))\n","\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":164317423,"sourceType":"kernelVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
