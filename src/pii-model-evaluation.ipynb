{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:04.300336Z","iopub.status.busy":"2024-02-26T17:30:04.299973Z","iopub.status.idle":"2024-02-26T17:30:04.306491Z","shell.execute_reply":"2024-02-26T17:30:04.305307Z","shell.execute_reply.started":"2024-02-26T17:30:04.300308Z"},"trusted":true},"outputs":[],"source":["import json\n","import argparse\n","from itertools import chain\n","import pandas as pd\n","from pathlib import Path\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n","from datasets import Dataset\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:04.311483Z","iopub.status.busy":"2024-02-26T17:30:04.311163Z","iopub.status.idle":"2024-02-26T17:30:16.319068Z","shell.execute_reply":"2024-02-26T17:30:16.317961Z","shell.execute_reply.started":"2024-02-26T17:30:04.311455Z"},"trusted":true},"outputs":[{"data":{"text/plain":["21672"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["data = json.load(open(\"../Dataset/data.json\"))\n","len(data)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:16.320604Z","iopub.status.busy":"2024-02-26T17:30:16.320312Z","iopub.status.idle":"2024-02-26T17:30:17.050647Z","shell.execute_reply":"2024-02-26T17:30:17.049464Z","shell.execute_reply.started":"2024-02-26T17:30:16.320578Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Samples in training data: 20322\n","Samples in validation data: 1350\n"]}],"source":["train, valid = [], []\n","\n","for row in data:\n","    if row[\"valid\"]: valid.append(row)\n","    else: train.append(row)\n","        \n","print(\"Samples in training data:\", len(train))\n","print(\"Samples in validation data:\", len(valid))"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:17.053139Z","iopub.status.busy":"2024-02-26T17:30:17.052717Z","iopub.status.idle":"2024-02-26T17:30:17.249333Z","shell.execute_reply":"2024-02-26T17:30:17.248352Z","shell.execute_reply.started":"2024-02-26T17:30:17.053112Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0 B-EMAIL\n","1 B-ID_NUM\n","2 B-NAME_STUDENT\n","3 B-PHONE_NUM\n","4 B-STREET_ADDRESS\n","5 B-URL_PERSONAL\n","6 B-USERNAME\n","7 I-ID_NUM\n","8 I-NAME_STUDENT\n","9 I-PHONE_NUM\n","10 I-STREET_ADDRESS\n","11 I-URL_PERSONAL\n","12 O\n"]}],"source":["all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}\n","\n","target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]\n","\n","for id, label in id2label.items():\n","    print(id, label)"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:17.250793Z","iopub.status.busy":"2024-02-26T17:30:17.250508Z","iopub.status.idle":"2024-02-26T17:30:17.260197Z","shell.execute_reply":"2024-02-26T17:30:17.259263Z","shell.execute_reply.started":"2024-02-26T17:30:17.250768Z"},"trusted":true},"outputs":[],"source":["def rebuild_from_example(example):\n","    text, labels, token_map = [], [], []\n","\n","    for idx, (t, l, ws) in enumerate(zip(\n","        example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]\n","    )):\n","        text.append(t)\n","        token_map.extend([idx] * len(t))\n","        labels.extend([l] * len(t))\n","\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","            token_map.append(-1)\n","            \n","    labels = np.array(labels)\n","    text = \"\".join(text)\n","    \n","    return text, labels, token_map"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:17.262081Z","iopub.status.busy":"2024-02-26T17:30:17.261504Z","iopub.status.idle":"2024-02-26T17:30:17.271310Z","shell.execute_reply":"2024-02-26T17:30:17.270551Z","shell.execute_reply.started":"2024-02-26T17:30:17.262049Z"},"trusted":true},"outputs":[],"source":["def tokenize(example, tokenizer, label2id, max_length):\n","    text, labels, token_map = rebuild_from_example(example)\n","\n","    # actual tokenization\n","    tokenized = tokenizer(text, \n","                          return_offsets_mapping=True, \n","                          max_length=max_length, \n","                          truncation=True)\n","    \n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0:\n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        token_labels.append(label2id[labels[start_idx]])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {\n","        **tokenized, \n","        \"labels\": token_labels, \n","        \"length\": length,\n","        \"token_map\": token_map\n","    }"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:17.272572Z","iopub.status.busy":"2024-02-26T17:30:17.272303Z","iopub.status.idle":"2024-02-26T17:30:17.285264Z","shell.execute_reply":"2024-02-26T17:30:17.284456Z","shell.execute_reply.started":"2024-02-26T17:30:17.272549Z"},"trusted":true},"outputs":[],"source":["def create_dict(data):\n","    keys = [\"full_text\", \"document\", \"tokens\", \"trailing_whitespace\", \"labels\", \"token_indices\"]\n","    \n","    # Initialize each key to have the same number of elements\n","    # as the number of rows in `data`\n","    output = {key: [None] * len(data) for key in keys}\n","    \n","    # Assign values to the dictionary\n","    for idx, row in enumerate(data):\n","        for key in keys:\n","            output[key][idx] = row[key]\n","    \n","    return output"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:17.286673Z","iopub.status.busy":"2024-02-26T17:30:17.286369Z","iopub.status.idle":"2024-02-26T17:30:17.295159Z","shell.execute_reply":"2024-02-26T17:30:17.294354Z","shell.execute_reply.started":"2024-02-26T17:30:17.286650Z"},"trusted":true},"outputs":[],"source":["INFERENCE_MAX_LENGTH = 2048\n","max_length = 1024\n","model_path = '../model_dir/deberta3base_1024'"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:17.296468Z","iopub.status.busy":"2024-02-26T17:30:17.296208Z","iopub.status.idle":"2024-02-26T17:30:39.362095Z","shell.execute_reply":"2024-02-26T17:30:39.360926Z","shell.execute_reply.started":"2024-02-26T17:30:17.296438Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"698f61bdadeb47e789f3d0992aa34fab","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=3):   0%|          | 0/1350 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","# train_ds = Dataset.from_dict(create_dict(train))\n","# train_ds = train_ds.map(tokenize, \n","#                         fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": max_length}, \n","#                         num_proc=3)\n","\n","valid_ds = Dataset.from_dict(create_dict(valid))\n","valid_ds = valid_ds.map(tokenize, \n","                        fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": max_length}, \n","                        num_proc=3)"]},{"cell_type":"markdown","metadata":{},"source":["### Compute Metrics"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:39.365997Z","iopub.status.busy":"2024-02-26T17:30:39.365692Z","iopub.status.idle":"2024-02-26T17:30:39.379275Z","shell.execute_reply":"2024-02-26T17:30:39.378223Z","shell.execute_reply.started":"2024-02-26T17:30:39.365968Z"},"trusted":true},"outputs":[],"source":["from typing import Dict\n","\n","class PRFScore:\n","    \"\"\"A precision / recall / F score.\"\"\"\n","\n","    def __init__(self, *, tp: int = 0, fp: int = 0, fn: int = 0) -> None:\n","        self.tp, self.fp, self.fn = tp, fp, fn\n","\n","    def __len__(self) -> int:\n","        return self.tp + self.fp + self.fn\n","\n","    def __iadd__(self, other):  # in-place add\n","        self.tp += other.tp\n","        self.fp += other.fp\n","        self.fn += other.fn\n","        return self\n","\n","    def __add__(self, other):\n","        return PRFScore(\n","            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n","        )\n","\n","    def score_set(self, cand: set, gold: set) -> None:\n","        self.tp += len(cand.intersection(gold))\n","        self.fp += len(cand - gold)\n","        self.fn += len(gold - cand)\n","\n","    @property\n","    def precision(self) -> float:\n","        return self.tp / (self.tp + self.fp + 1e-100)\n","\n","    @property\n","    def recall(self) -> float:\n","        return self.tp / (self.tp + self.fn + 1e-100)\n","\n","    @property\n","    def f1(self) -> float:\n","        p, r = self.precision, self.recall\n","        return 2 * ((p * r) / (p + r + 1e-100))\n","\n","    @property\n","    def f5(self) -> float:\n","        beta, p, r = 5, self.precision, self.recall\n","        fbeta = (1 + (beta**2)) * p * r / ((beta**2) * p + r + 1e-100)\n","        return fbeta\n","\n","    def to_dict(self) -> Dict[str, float]:\n","        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:39.380890Z","iopub.status.busy":"2024-02-26T17:30:39.380615Z","iopub.status.idle":"2024-02-26T17:30:39.396743Z","shell.execute_reply":"2024-02-26T17:30:39.395892Z","shell.execute_reply.started":"2024-02-26T17:30:39.380867Z"},"trusted":true},"outputs":[],"source":["def parse_predictions(predictions, id2label, ds, threshold=0.9):\n","    pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis=2).reshape(\n","        predictions.shape[0], predictions.shape[1], 1\n","    )\n","    preds = predictions.argmax(-1)\n","    preds_without_O = pred_softmax[:, :, :12].argmax(-1)\n","    O_preds = pred_softmax[:, :, 12]\n","    preds_final = np.where(O_preds < threshold, preds_without_O, preds)\n","    \n","    pairs = []\n","    row, document, token, label, token_str = [], [], [], [], []\n","    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(\n","        zip(\n","            preds_final,\n","            ds[\"token_map\"],\n","            ds[\"offset_mapping\"],\n","            ds[\"tokens\"],\n","            ds[\"document\"],\n","            ds[\"token_indices\"],\n","        )\n","    ):\n","\n","        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n","            label_pred = id2label[token_pred]\n","\n","            if start_idx + end_idx == 0:\n","                continue\n","\n","            if token_map[start_idx] == -1:\n","                start_idx += 1\n","\n","            # ignore \"\\n\\n\"\n","            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n","                start_idx += 1\n","\n","            if start_idx >= len(token_map):\n","                break\n","\n","            original_token_id = token_map[start_idx]\n","            token_id = indices[original_token_id]\n","            \n","            # ignore \"O\" predictions and whitespace preds\n","            if label_pred != \"O\" and token_id != -1:\n","                pair=(doc, token_id)\n","\n","                if pair not in pairs:\n","                    row.append(i)\n","                    document.append(doc)\n","                    token.append(token_id)\n","                    label.append(label_pred)\n","                    token_str.append(tokens[original_token_id])\n","                    pairs.append(pair)\n","                    \n","    df = pd.DataFrame(\n","        {\n","            \"eval_row\": row,\n","            \"document\": document,\n","            \"token\": token,\n","            \"label\": label,\n","            \"token_str\": token_str,\n","        }\n","    )\n","\n","    df = df.drop_duplicates().reset_index(drop=True)\n","\n","    df[\"row_id\"] = list(range(len(df)))\n","    return df"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:39.398774Z","iopub.status.busy":"2024-02-26T17:30:39.398125Z","iopub.status.idle":"2024-02-26T17:30:39.413134Z","shell.execute_reply":"2024-02-26T17:30:39.412452Z","shell.execute_reply.started":"2024-02-26T17:30:39.398742Z"},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","\n","def compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n","    predictions, labels = p\n","    \n","    pred_df = parse_predictions(predictions, id2label, valid_ds, threshold=threshold)\n","    \n","    references = {\n","        (row.document, row.token, row.label) # TODO: Change to pair\n","        for row in valid_df.itertuples()\n","    }\n","    predictions = {\n","        (row.document, row.token, row.label) # TODO: Change to pair\n","        for row in pred_df.itertuples()\n","    }\n","    \n","    score_per_type = defaultdict(PRFScore)\n","    references = set(references)\n","    \n","    for ex in predictions:\n","        pred_type = ex[-1] # (Document, token, label)\n","        \n","        if pred_type != \"O\":\n","            pred_type = pred_type[2:] # Discard B- and I- prefix\n","            \n","        if pred_type not in score_per_type:\n","            score_per_type[pred_type] = PRFScore()\n","            \n","        if ex in references:\n","            score_per_type[pred_type].tp += 1\n","            references.remove(ex)\n","        else:\n","            score_per_type[pred_type].fp += 1\n","            \n","    for _, _, ref_type in references: # Remaining labels not predicted\n","        if pred_type != \"O\":\n","            pred_type = pred_type[2:] # Discard B- and I- prefix\n","        \n","        if pred_type not in score_per_type:\n","            score_per_type[pred_type] = PRFScore()\n","            \n","        score_per_type[pred_type].fn += 1\n","        \n","    totals = PRFScore()\n","    \n","    for prf in score_per_type.values():\n","        totals += prf\n","        \n","    results = {\n","        \"ents_p\": totals.precision,\n","        \"ents_r\": totals.recall,\n","        \"ents_f5\": totals.f5,\n","        \"ents_per_type\": {\n","            k: v.to_dict() for k, v in score_per_type.items() if k != \"O\"\n","        },\n","    }\n","    \n","    final_results = {}\n","    for key, value in results.items():\n","        if isinstance(value, dict):\n","            for n, v in value.items():\n","                if isinstance(v, dict):\n","                    for n2, v2 in v.items():\n","                        final_results[f\"{key}_{n}_{n2}\"] = v2\n","                else:\n","                    final_results[f\"{key}_{n}\"] = v\n","        else:\n","            final_results[key] = value\n","\n","    return final_results"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:39.414382Z","iopub.status.busy":"2024-02-26T17:30:39.414124Z","iopub.status.idle":"2024-02-26T17:30:39.893955Z","shell.execute_reply":"2024-02-26T17:30:39.893182Z","shell.execute_reply.started":"2024-02-26T17:30:39.414349Z"},"trusted":true},"outputs":[],"source":["def get_reference_df(valid):\n","    raw_df = pd.DataFrame(valid)\n","    ref_df = raw_df[[\"document\", \"tokens\", \"labels\"]].copy()\n","    ref_df = (\n","        ref_df.explode([\"tokens\", \"labels\"])\n","        .reset_index(drop=True)\n","        .rename(columns={\"tokens\": \"token\", \"labels\": \"label\"})\n","    )\n","    ref_df[\"token\"] = ref_df.groupby(\"document\").cumcount()\n","\n","    reference_df = ref_df[ref_df[\"label\"] != \"O\"].copy()\n","    reference_df = reference_df.reset_index().rename(columns={\"index\": \"row_id\"})\n","    reference_df = reference_df[[\"row_id\", \"document\", \"token\", \"label\"]].copy()\n","\n","    return reference_df\n","\n","\n","reference_df = get_reference_df(valid)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Inferencing"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:39.895305Z","iopub.status.busy":"2024-02-26T17:30:39.895059Z","iopub.status.idle":"2024-02-26T17:30:45.577938Z","shell.execute_reply":"2024-02-26T17:30:45.577144Z","shell.execute_reply.started":"2024-02-26T17:30:39.895283Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(model_path)\n","collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n","args = TrainingArguments(\n","    \".\", \n","    per_device_eval_batch_size=1, \n","    report_to=\"none\",\n",")\n","trainer = Trainer(\n","    model=model, \n","    args=args, \n","    data_collator=collator, \n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:30:45.579267Z","iopub.status.busy":"2024-02-26T17:30:45.578987Z","iopub.status.idle":"2024-02-26T17:32:04.966147Z","shell.execute_reply":"2024-02-26T17:32:04.965184Z","shell.execute_reply.started":"2024-02-26T17:30:45.579243Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"599076cf6a434454aec1cd55829477ea","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20322 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["preds = trainer.predict(valid_ds)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:32:04.967942Z","iopub.status.busy":"2024-02-26T17:32:04.967587Z","iopub.status.idle":"2024-02-26T17:33:33.962386Z","shell.execute_reply":"2024-02-26T17:33:33.961489Z","shell.execute_reply.started":"2024-02-26T17:32:04.967909Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing final metrics...\n","{'final_f5_at_0.5': 0.8332093840849165, 'final_f5_at_0.6': 0.8329960879535949, 'final_f5_at_0.7': 0.8323917137476459, 'final_f5_at_0.8': 0.8295652173913043, 'final_f5_at_0.9': 0.8256430654998673, 'final_f5_at_0.95': 0.8159067085953878, 'final_f5_at_0.97': 0.8050420168067227}\n"]}],"source":["print(\"Computing final metrics...\")\n","final_metrics = {\n","    f\"final_f5_at_{threshold}\": compute_metrics(\n","        (preds.predictions, None),\n","        id2label,\n","        valid_ds,\n","        reference_df,\n","        threshold=threshold,\n","    )[\"ents_f5\"]\n","    for threshold in [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.97]\n","}\n","print(final_metrics)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T17:33:33.964449Z","iopub.status.busy":"2024-02-26T17:33:33.963733Z","iopub.status.idle":"2024-02-26T17:33:33.969397Z","shell.execute_reply":"2024-02-26T17:33:33.968481Z","shell.execute_reply.started":"2024-02-26T17:33:33.964395Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5\n"]}],"source":["best_threshold = float(max(final_metrics, key=final_metrics.get).split(\"_\")[-1])\n","print(best_threshold)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"sourceId":164317423,"sourceType":"kernelVersion"},{"sourceId":164394118,"sourceType":"kernelVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
