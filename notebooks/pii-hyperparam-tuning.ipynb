{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from functools import partial\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    set_seed,\n",
    ")\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metric import (\n",
    "    compute_metrics,\n",
    "    get_f5_at_different_thresholds,\n",
    ")\n",
    "from src.data import create_dataset\n",
    "from src.utils import (\n",
    "    get_reference_df_parquet,\n",
    "    parse_predictions,\n",
    "    filter_errors,\n",
    "    generate_htmls_concurrently,\n",
    "    visualize,\n",
    "    convert_for_upload,\n",
    "    CustomTrainer,\n",
    "    parse_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024\n",
    "WANDB_PROJECT = \"Kaggle-PII\"\n",
    "USER_NAME = \"shakleenishfar\"\n",
    "PROJECT_PATH = f\"laplacesdemon43/{WANDB_PROJECT}\"\n",
    "EXPERIMENT = f\"pii-sweep-001\"\n",
    "WANDB_NAME = f\"DeBERTA-v3-{MODEL_SIZE}-{MAX_LENGTH}-Sweep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshakleenishfar\u001b[0m (\u001b[33mlaplacesdemon43\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ishfar/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"0bf204609ea345c7c595565d736a9d62ca69f838\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    # How to perform hyperparameter tuning\n",
    "    \"method\": \"Bayesian\",\n",
    "    # How to evaluate which hyperparameter combination is good\n",
    "    \"metric\": {\n",
    "        \"name\": \"ents_f5\",\n",
    "        \"goal\": \"maximize\",\n",
    "    },\n",
    "    # Hyperparameters to tune\n",
    "    \"parameters\": {\n",
    "        # Hyperparameters that will change\n",
    "        \"o_weight\": {\"distribution\": \"uniform\", \"min\": 0.2, \"max\": 0.35},\n",
    "        \"learning_rate\": {\"distribution\": \"uniform\", \"min\": 2e-5, \"max\": 1e-4},\n",
    "        \"weight_decay\": {\"distribution\": \"uniform\", \"min\": 0.02, \"max\": 0.08},\n",
    "        \"num_train_epochs\": {\"value\": 3},\n",
    "        \"warmup_ratio\": {\"value\": 0.1},\n",
    "        # Hyperparameters that will not change\n",
    "        \"threshold\": {\"value\": 0.95},\n",
    "        \"stride_artifact\": {\"value\": f\"{PROJECT_PATH}/processed_data:v0\"},\n",
    "        \"raw_artifact\": {\"value\": f\"{PROJECT_PATH}/raw_data:v0\"},\n",
    "        \"output_dir\": {\"value\": f\"model_dir/DeBERTA-V3-{MODEL_SIZE}-{MAX_LENGTH}\"},\n",
    "        \"inference_max_length\": {\"value\": 1024},\n",
    "        \"training_max_length\": {\"value\": 1024},\n",
    "        \"training_model_path\": {\"value\": f\"microsoft/deberta-v3-{MODEL_SIZE}\"},\n",
    "        \"fp16\": {\"value\": True},\n",
    "        \"per_device_train_batch_size\": {\"value\": 8},\n",
    "        \"per_device_eval_batch_size\": {\"value\": 8},\n",
    "        \"evaluation_strategy\": {\"value\": \"no\"},\n",
    "        \"do_eval\": {\"value\": False},\n",
    "        \"save_total_limit\": {\"value\": 1},\n",
    "        \"logging_steps\": {\"value\": 10},\n",
    "        \"lr_scheduler_type\": {\"value\": \"cosine\"},\n",
    "        \"random_state\": {\"value\": 29},\n",
    "        \"gradient_accumulation_steps\": {\"value\": 2},\n",
    "    },\n",
    "    # Early stopping\n",
    "    # \"early_terminate\": {\n",
    "    #     \"type\": \"hyperband\",\n",
    "    #     \"max_iter\": 27,\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: pjbq0w1i\n",
      "Sweep URL: https://wandb.ai/laplacesdemon43/Kaggle-PII/sweeps/pjbq0w1i\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    stride_artifact = wandb.use_artifact(config.stride_artifact)\n",
    "    stride_artifact_dir = stride_artifact.download()\n",
    "    df = pd.read_parquet(stride_artifact_dir + \"/stride_data.parquet\")\n",
    "\n",
    "    train_df = df[df.valid == False].reset_index(drop=True)\n",
    "    eval_df = df[df.valid == True].reset_index(drop=True)\n",
    "\n",
    "    negatives, positives = [], []\n",
    "\n",
    "    for _, row in train_df.iterrows():\n",
    "        if any(row.labels != \"O\"):\n",
    "            positives.append(row)\n",
    "        else:\n",
    "            negatives.append(row)\n",
    "\n",
    "    positives, negatives = pd.DataFrame(positives), pd.DataFrame(negatives)\n",
    "    negatives = negatives.iloc[: negatives.shape[0] // 3]\n",
    "    train_df = pd.concat([positives, negatives])\n",
    "    train_df = train_df.sample(frac=1, random_state=config.random_state)\n",
    "\n",
    "    reference_df = get_reference_df_parquet(config.raw_artifact)\n",
    "\n",
    "    all_labels = sorted(list(set(chain(*[x.tolist() for x in df.labels.values]))))\n",
    "    label2id = {l: i for i, l in enumerate(all_labels)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "    return train_df, eval_df, reference_df, all_labels, label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_dataset(config, train_df, eval_df, label2id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.training_model_path)\n",
    "    train_ds = create_dataset(train_df, tokenizer, config.training_max_length, label2id)\n",
    "    valid_ds = create_dataset(eval_df, tokenizer, config.inference_max_length, label2id)\n",
    "    return tokenizer, train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    config,\n",
    "    all_labels,\n",
    "    id2label,\n",
    "    label2id,\n",
    "    tokenizer,\n",
    "    train_ds,\n",
    "    valid_ds,\n",
    "    reference_df,\n",
    "):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        config.training_model_path,\n",
    "        num_labels=len(all_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=config.output_dir,\n",
    "        fp16=config.fp16,\n",
    "        learning_rate=config.learning_rate,\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        evaluation_strategy=config.evaluation_strategy,\n",
    "        do_eval=config.do_eval,\n",
    "        save_total_limit=config.save_total_limit,\n",
    "        logging_steps=config.logging_steps,\n",
    "        lr_scheduler_type=config.lr_scheduler_type,\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        weight_decay=config.weight_decay,\n",
    "    )\n",
    "\n",
    "    class_weights = torch.tensor([1.0] * 12 + [config.o_weight]).to(\"cuda\")\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=None,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=partial(\n",
    "            compute_metrics,\n",
    "            id2label=id2label,\n",
    "            valid_ds=valid_ds,\n",
    "            valid_df=reference_df,\n",
    "            threshold=config.threshold,\n",
    "        ),\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_valid_f5(config, trainer, id2label, valid_ds, reference_df):\n",
    "    preds = trainer.predict(valid_ds)\n",
    "    metric = compute_metrics(\n",
    "        (preds.predictions, None),\n",
    "        id2label,\n",
    "        valid_ds,\n",
    "        reference_df,\n",
    "        config.threshold,\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"ents_f5\": metric[\"ents_f5\"],\n",
    "            \"ents_r\": metric[\"ents_r\"],\n",
    "            \"ents_p\": metric[\"ents_p\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Set random seed\n",
    "        set_seed(config.random_state)\n",
    "        torch.manual_seed(config.random_state)\n",
    "\n",
    "        train_df, eval_df, reference_df, all_labels, label2id, id2label = get_data(\n",
    "            config\n",
    "        )\n",
    "\n",
    "        tokenizer, train_ds, valid_ds = get_tokenized_dataset(\n",
    "            config,\n",
    "            train_df,\n",
    "            eval_df,\n",
    "            label2id,\n",
    "        )\n",
    "\n",
    "        trainer = train(\n",
    "            config,\n",
    "            all_labels,\n",
    "            id2label,\n",
    "            label2id,\n",
    "            tokenizer,\n",
    "            train_ds,\n",
    "            valid_ds,\n",
    "            reference_df,\n",
    "        )\n",
    "\n",
    "        calculate_valid_f5(config, trainer, id2label, valid_ds, reference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vf3visoy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdo_eval: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tevaluation_strategy: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfp16: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinference_max_length: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001949620073745712\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: cosine\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \to_weight: 0.5304613681790084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dir: model_dir/DeBERTA-V3-base-1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_eval_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \traw_artifact: laplacesdemon43/Kaggle-PII/raw_data:v0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_total_limit: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride_artifact: laplacesdemon43/Kaggle-PII/processed_data:v0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tthreshold: 0.95\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_max_length: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_model_path: microsoft/deberta-v3-base\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1967696561199332\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/ishfar/New Volume/Studies/Projects/Kaggle/PII_Detection/wandb/run-20240308_180922-vf3visoy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/vf3visoy' target=\"_blank\">lunar-sweep-1</a></strong> to <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/sweeps/pjbq0w1i' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII/sweeps/pjbq0w1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/sweeps/pjbq0w1i' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII/sweeps/pjbq0w1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/vf3visoy' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/vf3visoy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9806/4283551679.py\", line 8, in main\n",
      "    if train_df is None:\n",
      "       ^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'train_df' where it is not associated with a value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711de921e807489ea7fbc0ccf7d60981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-sweep-1</strong> at: <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/vf3visoy' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/vf3visoy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240308_180922-vf3visoy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run vf3visoy errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/ishfar/New Volume/Studies/Projects/Kaggle/PII_Detection/venv/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_9806/4283551679.py\", line 8, in main\n",
      "    if train_df is None:\n",
      "       ^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'train_df' where it is not associated with a value\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run vf3visoy errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/media/ishfar/New Volume/Studies/Projects/Kaggle/PII_Detection/venv/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_9806/4283551679.py\", line 8, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     if train_df is None:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m        ^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m UnboundLocalError: cannot access local variable 'train_df' where it is not associated with a value\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mclp7a6c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdo_eval: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tevaluation_strategy: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfp16: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinference_max_length: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.468656942723983e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: cosine\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \to_weight: 0.16734010883153114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dir: model_dir/DeBERTA-V3-base-1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_eval_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \traw_artifact: laplacesdemon43/Kaggle-PII/raw_data:v0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_total_limit: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride_artifact: laplacesdemon43/Kaggle-PII/processed_data:v0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tthreshold: 0.95\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_max_length: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_model_path: microsoft/deberta-v3-base\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1684870225916143\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/ishfar/New Volume/Studies/Projects/Kaggle/PII_Detection/wandb/run-20240308_180933-mclp7a6c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/mclp7a6c' target=\"_blank\">upbeat-sweep-2</a></strong> to <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/sweeps/pjbq0w1i' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII/sweeps/pjbq0w1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/sweeps/pjbq0w1i' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII/sweeps/pjbq0w1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/mclp7a6c' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/mclp7a6c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9806/4283551679.py\", line 8, in main\n",
      "    if train_df is None:\n",
      "       ^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'train_df' where it is not associated with a value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ebb4ed9e72437c8c47b3d42369cca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-sweep-2</strong> at: <a href='https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/mclp7a6c' target=\"_blank\">https://wandb.ai/laplacesdemon43/Kaggle-PII/runs/mclp7a6c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240308_180933-mclp7a6c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run mclp7a6c errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/ishfar/New Volume/Studies/Projects/Kaggle/PII_Detection/venv/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_9806/4283551679.py\", line 8, in main\n",
      "    if train_df is None:\n",
      "       ^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'train_df' where it is not associated with a value\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run mclp7a6c errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/media/ishfar/New Volume/Studies/Projects/Kaggle/PII_Detection/venv/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_9806/4283551679.py\", line 8, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     if train_df is None:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m        ^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m UnboundLocalError: cannot access local variable 'train_df' where it is not associated with a value\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, main, count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
